{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMc/abhIU2wI1Z1pQtkrcTy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V29CQxTcJRSm"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "try:\n",
        "    subprocess.check_output('nvidia-smi')\n",
        "    print(\"a GPU is connected.\")\n",
        "except Exception:\n",
        "    # TPU or CPU\n",
        "    if \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "      print(\"A TPU is connected.\")\n",
        "      import jax.tools.colab_tpu\n",
        "      jax.tools.colab_tpu.setup_tpu()\n",
        "    else:\n",
        "      print(\"Only CPU accelerator is connected.\")\n",
        "      # x8 cpu devices - number of (emulated) host devices\n",
        "      os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def plot_performance(data: Dict, title: str) -> None:\n",
        "    runs = list(data.keys())\n",
        "    time = list(data.values())\n",
        "\n",
        "\n",
        "    plt.bar(runs, time, width = 0.4)\n",
        "\n",
        "    plt.xlabel(\"Implementation\")\n",
        "    plt.ylabel(\"Average time taken (s)\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    best_perf_key = min(data, key = data.get)\n",
        "    all_runs_key = copy.copy(runs)\n",
        "\n",
        "\n",
        "    all_runs_key.remove(best_perf_key)\n",
        "\n",
        "    for k in all_runs_key:\n",
        "        print(\n",
        "            f\"{best_perf_key} was {round(data[best_perf_key] / data[k], 2)}x faster than {k}\"\n",
        "        )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NEbLge6RKHRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")\n"
      ],
      "metadata": {
        "id": "t8esUlClKkh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX = Autograd + XLA (accelerated linear algebra)"
      ],
      "metadata": {
        "id": "KupiYglxK-zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* automatic differentation (grad)\n",
        "* parallelization (pmap)\n",
        "* vectorization (vmap)\n",
        "* just-in-time compilation (jit)\n",
        "\n",
        "* XLA allows for accelerator agnostic computing"
      ],
      "metadata": {
        "id": "-eImf_PoLEJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy version\n",
        "x = np.linspace(-np.pi, np.pi, 100)\n",
        "\n",
        "y = np.sin(x)\n",
        "\n",
        "plt.plot(x,y, \"b\", label = \"y = sin (x)\")\n",
        "plt.legend(loc = \"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BQnomoc_Ku6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jax version\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "y = jnp.sin(x)\n",
        "\n",
        "plt.plot(x,y, \"b\", label = \"y = sin (x)\")\n",
        "plt.legend(loc = \"best\")\n",
        "plt.show()\n",
        "\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "y = jnp.cos(x)\n",
        "\n",
        "plt.plot(x,y, \"r\", label = \"y = cos (x)\")\n",
        "plt.legend(loc = \"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PoP5GcqNLhdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the differences between JAX and NumPy\n",
        "* JAX arrays are immmutable\n",
        "* Jax handles randomness EXPLICITLY\n"
      ],
      "metadata": {
        "id": "0Z2S1GgIL0tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example in NumPy\n",
        "# mutuable arrays\n",
        "x= np.arange(10)\n",
        "x[0] = 10\n",
        "print(x)\n",
        "#"
      ],
      "metadata": {
        "id": "-FHcsGsBMIn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX issue\n",
        "try:\n",
        "    x = jnp.arange(10)\n",
        "    x[0] = 10\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))\n"
      ],
      "metadata": {
        "id": "hDshg8mXMUDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# solution\n",
        "\n",
        "\n",
        "x = jnp.arange(10)\n",
        "new_x = x.at[0].set(10) # new_x is now a copy of the original x\n",
        "print(f\" new_x: {new_x} original x: {x}\")"
      ],
      "metadata": {
        "id": "y_DcISjEMbNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomness in JAX\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "prng_state = np.random.get_state()\n",
        "\n",
        "def is_prng_state_the_same(prng_1, prng_2):\n",
        "    \"\"\"Helper function to compare two prng keys.\"\"\"\n",
        "    # concat all elements in prng tuple\n",
        "    list_prng_data_equal = [(a == b) for a, b in zip(prng_1, prng_2)]\n",
        "    # stack all elements together\n",
        "    list_prng_data_equal = np.hstack(list_prng_data_equal)\n",
        "    # check if all elements are the same\n",
        "    is_prng_equal = all(list_prng_data_equal)\n",
        "    return is_prng_equal\n",
        "\n",
        "print(\n",
        "    f\"sample 1 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 2 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 3 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "\n",
        "### Numpy's global random state is updated every time a new random num is generated.... We don't want this because we want to handle randomness ina  REPRODUCIBLE way across different threads/processes/devices.\n",
        "\n"
      ],
      "metadata": {
        "id": "AtZ394pLMfAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "print(f\"sample 2 = {random.normal(key)}\")\n",
        "print(f\"sample 3 = {random.normal(key)}\")"
      ],
      "metadata": {
        "id": "6oE6LD6pNGLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "\n",
        "# We split the key -> new key and subkey\n",
        "new_key, subkey = random.split(key)\n",
        "\n",
        "# We use the subkey immediately and keep the new key for future splits.\n",
        "# It doesn't really matter which key we keep and which one we use immediately.\n",
        "print(f\"sample 2 = {random.normal(subkey)}\")\n",
        "\n",
        "# We split the new key -> new key2 and subkey\n",
        "new_key2, subkey = random.split(new_key)\n",
        "print(f\"sample 3 = {random.normal(subkey)}\")"
      ],
      "metadata": {
        "id": "ettHDyR9NI7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating a dot product in Numpy on CPU\n",
        "\n",
        "size = 1000\n",
        "x = np.random.normal(size = (size, size))\n",
        "y = np.random.normal(size = (size, size))\n",
        "\n",
        "numpy_time = %timeit -o -n 10 a_np = np.dot(x,y)\n",
        "\n"
      ],
      "metadata": {
        "id": "YmpXDWYmNVkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "size = 1000\n",
        "key1, key2 = jax.random.split(jax.random.PRNGKey(42), num=2)\n",
        "x = jax.random.normal(key1, shape=(size, size))\n",
        "y = jax.random.normal(key2, shape=(size, size))\n",
        "jax_time = %timeit -o -n 10 jnp.dot(y, x.T).block_until_ready()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KyQWQByzNmHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "JAX Transformations\n"
      ],
      "metadata": {
        "id": "i2RgUgTdNvJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jit\n",
        "\n",
        "def relu(x):\n",
        "    return jnp.maximum(0, x)\n",
        "\n",
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "print(relu_jit(jnp.array([-1, 0, 1])))\n"
      ],
      "metadata": {
        "id": "C1ByIRaLNxXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grad\n",
        "\n",
        "f = lambda x: 6* x**5 - 4*x**3 + 2*x**2 - 1\n",
        "\n",
        "dfdx = jax.grad(f)\n",
        "print(dfdx(2.))"
      ],
      "metadata": {
        "id": "MhxkEX-mPrf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vmap\n",
        "\n",
        "def min_max(x):\n",
        "    return jnp.array([jnp.min(x), jnp.max(x)])\n",
        "\n",
        "batch_size = 3\n",
        "batched_x = np.arange(15).reshape((batch_size, -1))\n",
        "print(batched_x)\n",
        "\n",
        "print(min_max(batched_x))\n",
        "\n",
        "def vmap_min_max(x):\n",
        "    return vmap(min_max)(x)\n",
        "\n",
        "print(vmap_min_max(batched_x))\n",
        "\n"
      ],
      "metadata": {
        "id": "DA0RHz3vP9j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def manual_batch_min_max_loop(batched_x):\n",
        "    min_max_result_list = []\n",
        "    for x in batched_x:\n",
        "        min_max_result_list.append(min_max(x))\n",
        "    return jnp.array(min_max_result_list)\n",
        "\n",
        "print(manual_batch_min_max_loop(batched_x))\n",
        "\n",
        "\n",
        "@jit\n",
        "def manual_batch_min_max_axis(batched_x):\n",
        "    return jnp.stack([jnp.min(batched_x, axis = 1), jnp.max(batched_x, axis = 1)])\n",
        "\n",
        "print(manual_batch_min_max_axis(batched_x))"
      ],
      "metadata": {
        "id": "BqnWGuFA3hiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vmap\n",
        "\n",
        "@jit\n",
        "def min_max_vmap(batched_x):\n",
        "    return vmap(min_max)(batched_x)\n",
        "\n",
        "## We add extra dimention in a single vector, shape changes from (5,) to (1,5), which makes the vmapping possible\n",
        "x_with_extra_dim = jax.numpy.expand_dims(batched_x, axis = 0)\n",
        "print(min_max_vmap(x_with_extra_dim))\n"
      ],
      "metadata": {
        "id": "4-Q0UKjm4LuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pmap\n",
        "\n",
        "### TO DO"
      ],
      "metadata": {
        "id": "Q5f-FlOJ4iRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lin Regression"
      ],
      "metadata": {
        "id": "PNgyrlOQ41QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# simple toy dataset\n",
        "\n",
        "\n",
        "\n",
        "x_data_list = [210, 160, 240, 140, 300]\n",
        "y_data_list = [4, 3.3, 3.7, 2.3, 5.4]\n",
        "\n",
        "\n",
        "def loss_function(b,w):\n",
        "    f = w*x + b\n",
        "    errors = jnp.square(y-f)\n",
        "\n",
        "    return 1 / 2 * jnp.mean(errors)\n",
        "\n",
        "\n",
        "auto_grad = jax.grad(loss_function, argnums = (0,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def manual_grad(b, w):\n",
        "    grad_b = 0\n",
        "    grad_w = 0\n",
        "    for x, y in zip(x_data_list, y_data_list):\n",
        "        f = w * x + b\n",
        "        grad_b += f - y\n",
        "        grad_w += (f - y) * x\n",
        "    grad_b /= len(x_data_list)\n",
        "    grad_w /= len(x_data_list)\n",
        "    return grad_b, grad_w\n",
        "\n",
        "b, w = 2.5, 3.5\n",
        "grad_b_autograd, grad_w_autograd = auto_grad(b, w)\n",
        "print(\"Autograd         grad_b:\", grad_b_autograd, \"  grad_w\", grad_w_autograd)\n",
        "\n",
        "grad_b_manual, grad_w_manual = manual_grad(b, w)\n",
        "print(\"Manual gradients grad_b:\", grad_b_manual, \"  grad_w\", grad_w_manual)\n"
      ],
      "metadata": {
        "id": "FkV_4q_k4mHn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}